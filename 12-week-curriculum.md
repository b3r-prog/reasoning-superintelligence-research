# ðŸ“… 12-Week Deep Dive Reading Schedule

> A structured curriculum for going from intermediate ML knowledge to research-frontier understanding of reasoning and superintelligence.

---

## Week 1â€“2: Foundations of Modern LLMs

**Goal:** Understand the transformer architecture, pretraining, and basic scaling.

| Day | Reading | Time |
|-----|---------|------|
| Mon | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | 2h |
| Tue | [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) | 1h |
| Wed | [Scaling Laws for Neural LMs](https://arxiv.org/abs/2001.08361) | 2h |
| Thu | [Chinchilla (Training Compute-Optimal LLMs)](https://arxiv.org/abs/2203.15556) | 2h |
| Fri | [Emergent Abilities of LLMs](https://arxiv.org/abs/2206.07682) | 1.5h |
| Weekend | Karpathy's [makemore](https://www.youtube.com/watch?v=PaCmpygFfXo) and [nanoGPT](https://github.com/karpathy/nanoGPT) | 4h |

---

## Week 3â€“4: Reasoning & Chain-of-Thought

**Goal:** Understand how LLMs reason and how to elicit better reasoning.

| Day | Reading | Time |
|-----|---------|------|
| Mon | [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903) | 1.5h |
| Tue | [Zero-Shot CoT ("Let's think step by step")](https://arxiv.org/abs/2205.11916) | 1h |
| Wed | [Self-Consistency](https://arxiv.org/abs/2203.11171) | 1.5h |
| Thu | [Tree of Thoughts](https://arxiv.org/abs/2305.10601) | 2h |
| Fri | [Least-to-Most Prompting](https://arxiv.org/abs/2205.10625) | 1h |
| Weekend | Implement a ToT solver for a simple logic puzzle | 4h |

---

## Week 5â€“6: Alignment & RLHF

**Goal:** Understand how LLMs are aligned with human preferences.

| Day | Reading | Time |
|-----|---------|------|
| Mon | [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565) | 2h |
| Tue | [InstructGPT / RLHF](https://arxiv.org/abs/2203.02155) | 2h |
| Wed | [Constitutional AI](https://arxiv.org/abs/2212.08073) | 2h |
| Thu | [DPO](https://arxiv.org/abs/2305.18290) | 1.5h |
| Fri | [Reward Overoptimization](https://arxiv.org/abs/2210.10760) | 1.5h |
| Weekend | Run a DPO fine-tune with [TRL](https://github.com/huggingface/trl) | 4h |

---

## Week 7â€“8: Mechanistic Interpretability

**Goal:** Understand how transformers compute, and how to "see inside" them.

| Day | Reading | Time |
|-----|---------|------|
| Mon | [Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) | 3h |
| Tue | [In-context Learning and Induction Heads](https://arxiv.org/abs/2209.11895) | 2h |
| Wed | [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) | 2h |
| Thu | [Towards Monosemanticity](https://transformer-circuits.pub/2023/monosemanticity/index.html) | 3h |
| Fri | [Scaling Sparse Autoencoders](https://arxiv.org/abs/2406.04093) | 2h |
| Weekend | Run [TransformerLens](https://github.com/neelnanda-io/TransformerLens) on GPT-2 | 4h |

---

## Week 9â€“10: Multi-Agent & Agentic Systems

**Goal:** Understand how agents plan, act, and collaborate.

| Day | Reading | Time |
|-----|---------|------|
| Mon | [ReAct](https://arxiv.org/abs/2210.03629) | 1.5h |
| Tue | [Reflexion](https://arxiv.org/abs/2303.11366) | 1.5h |
| Wed | [Generative Agents](https://arxiv.org/abs/2304.03442) | 2h |
| Thu | [AutoGen](https://arxiv.org/abs/2308.08155) | 1.5h |
| Fri | [Cognitive Architectures for Language Agents (CoALA)](https://arxiv.org/abs/2309.02427) | 2h |
| Weekend | Build a multi-agent pipeline with AutoGen | 4h |

---

## Week 11â€“12: Frontier Research & Open Problems

**Goal:** Engage with cutting-edge research and form your own research hypotheses.

| Day | Reading | Time |
|-----|---------|------|
| Mon | [Weak-to-Strong Generalization](https://arxiv.org/abs/2312.09390) | 2h |
| Tue | [Sleeper Agents](https://arxiv.org/abs/2401.05566) | 2h |
| Wed | [DeepSeek-R1](https://arxiv.org/abs/2501.12948) | 2.5h |
| Thu | [Scaling Test-Time Compute](https://arxiv.org/abs/2408.03314) | 2h |
| Fri | [AlphaGeometry 2 blog](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) | 1.5h |
| Weekend | Write a 2-page research proposal on an open problem from this hub | 4h |

---

## Supplementary: On-Demand Resources

- **For Math Background:** [Mathematics for ML](https://mml-book.github.io/)
- **For Coding:** [Andrej Karpathy's YouTube](https://www.youtube.com/@AndrejKarpathy)
- **For Theory:** [Understanding Deep Learning (Prince)](https://udlbook.github.io/udlbook/)
- **For Community:** [Alignment Forum](https://www.alignmentforum.org/), [EleutherAI Discord](https://discord.gg/eleutherai)

---

*This schedule is aggressive. Adjust pace to your background. The goal is breadth first, then depth.*
